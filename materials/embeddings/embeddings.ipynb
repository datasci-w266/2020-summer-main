{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters and Distributions\n",
    "\n",
    "We'll work through how to build and factorize a co-occurrence matrix, and do some simple visualization of the embeddings.\n",
    "\n",
    "On Assignment 3 and 4, we'll dig a bit deeper into the properties of these embeddings, and experiment with them on a classification task.\n",
    "\n",
    "**Note:** If viewing on GitHub, please use this NBViewer link for proper rendering: http://nbviewer.jupyter.org/github/datasci-w266/2020-summer-main/blob/master/materials/embeddings/embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "# Standard python helper libraries.\n",
    "import os, sys, time, shutil\n",
    "import itertools, collections\n",
    "from IPython.display import display\n",
    "\n",
    "# NumPy and SciPy for matrix ops\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "# NLTK for NLP utils\n",
    "import nltk\n",
    "\n",
    "# Helper libraries.\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Bokeh for plotting.\n",
    "utils.require_package(\"bokeh\")\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import LabelSet, HoverTool, WheelZoomTool\n",
    "bp.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll use the Brown corpus as our dataset, and do our usual simple preprocessing. Since we're just going to explore the embeddings, we don't need a train/dev/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(nltk.download('brown'))  # make sure we have the data\n",
    "corpus = nltk.corpus.brown\n",
    "vocab = vocabulary.Vocabulary(utils.canonicalize_word(w) for w in utils.flatten(corpus.sents()))\n",
    "print(\"Vocabulary: {:,} words\".format(vocab.size))\n",
    "\n",
    "tokens = utils.preprocess_sentences(corpus.sents(), vocab, use_eos=False, emit_ids=False)\n",
    "print(\"Corpus: {:,} tokens (counting <s>)\".format(len(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Co-occurrence Matrix\n",
    "\n",
    "The base for our word embeddings will be a co-occurrence matrix $M$. In the most general form, we'll consider this to be a **word-context matrix**, where the row indices $i$ correspond to words (types) $w_i$ in the vocabulary. Context could be:\n",
    "\n",
    "- Documents\n",
    "- Paragraphs or sentences\n",
    "- Syntactic contexts\n",
    "- Topics\n",
    "- Nearby words\n",
    "\n",
    "We're really interested in the words, so we're going to jump right to the last one. How do we define \"nearby\"? The simplest way is to just position: we'll define a *window* and say that two words co-occur if they appear in this window. For example:\n",
    "```\n",
    "the quick brown fox jumped over the lazy dog\n",
    "```\n",
    "With a window of $\\pm 2$ words, we say that `brown`, `fox`, `over`, and `the` are in the context of `jumped`, and so in our co-occurence matrix $C \\in M^{|V|\\times|V|}$ we have $C_{\\mathtt{brown,jumped}} = 1$, $C_{\\mathtt{fox,jumped}} = 1$, and so on.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Note:**_ It turns out that we can transform any word-context matrix $M$ into a word-word matrix:\n",
    "\n",
    "Let \n",
    "$$ M_{i\\ell} = \\mathbf{Count}[w_i \\in \\text{context}\\ \\ell] $$ \n",
    "\n",
    "Then for $i \\ne j$:\n",
    "\n",
    "$$ (MM^T)_{ij} = \\sum_{\\ell} M_{i\\ell} M_{j\\ell} = \\mathbf{Count}[w_i \\text{ in same context as } w_j] = C_{ij} $$\n",
    "\n",
    "There's a correction we'd need to do for the diagonal, but it won't change the structure of the representations that we get via the SVD. So regardless of the underlying context type, it's common to just deal with a word-word cooccurence matrix $C_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Co-occurrence Matrix\n",
    "\n",
    "In order to put our words in a matrix, we need to assign each one to a row index. Fortunately, our `Vocabulary` class does this automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = vocab.words_to_ids(tokens)\n",
    "print(\"Sample words: \" + str(tokens[:5]))\n",
    "print(\"Sample ids:   \" + str(token_ids[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our co-occurence counts are pairwise between words, so we'll want to have a sparse representation. The total number of matrix elements is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = vocab.size\n",
    "print(\"Total matrix elements: {:,} x {:,} = {:,}\".format(V, V, V**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as with bigrams, most of these will be zero. So, we'll define $C$ as a `scipy.sparse` matrix. Like the sparse dicts we used in the [language modeling demo](../../materials/simple_lm/lm1.py), the sparse matrix will only store the nonzero elements we need.\n",
    "\n",
    "_**Mathematical note:**_  \n",
    "We can compute each element by sliding a window over each position $\\ell$ in the corpus. Suppose our window is size $W = 2K + 1$. Then:\n",
    "\n",
    "$$ C_{ij} = \\sum_\\ell^{|\\text{tokens}|} \\sum_{k \\in [-K,K],\\ \\delta \\ne 0 } \\mathbf{1}[w_\\ell = i \\text{ and } w_{\\ell+k} = j] $$\n",
    "\n",
    "We'll hack this a little bit and change the order of the sum, which makes for simpler code:\n",
    "\n",
    "$$ C_{ij} = \\sum_{k \\in [-K,K],\\ k \\ne 0 } \\sum_\\ell^{|\\text{tokens}|} \\mathbf{1}[w_\\ell = i \\text{ and } w_{\\ell+k} = j] $$\n",
    "\n",
    "Conveniently, the above is symmetric, so we'll simplify further to:\n",
    "\n",
    "$$ C_{ij}^+ = \\sum_{k = 1}^K \\sum_\\ell^{|\\text{tokens}|} \\mathbf{1}[w_\\ell = i \\text{ and } w_{\\ell+k} = j] = \\sum_{k = 1}^K C_{ij}^+(k)$$\n",
    "\n",
    "$$ C_{ij}^- = \\sum_{k = -K}^1 \\sum_\\ell^{|\\text{tokens}|} \\mathbf{1}[w_\\ell = i \\text{ and } w_{\\ell+k} = j] = \\sum_{k = -K}^1 C_{ij}^-(k)$$\n",
    "\n",
    "It's easy to see that $C_{ij} = C_{ij}^+ + C_{ij}^-$, and since $C_{ij}^+ = C_{ji}^-$, $C$ is a symmetric matrix.\n",
    "\n",
    "Now we can write the formula in code, where our outer loop sums over $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurrence_matrix(token_ids, V, K=2):\n",
    "    # We'll use this as an \"accumulator\" matrix\n",
    "    C = scipy.sparse.csc_matrix((V,V), dtype=np.float32)\n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        print(u\"Counting pairs (i, i \\u00B1 {:d}) ...\".format(k))\n",
    "        i = token_ids[:-k]  # current word\n",
    "        j = token_ids[k:]   # k words ahead\n",
    "        data = (np.ones_like(i), (i,j))  # values, indices\n",
    "        Ck_plus = scipy.sparse.csc_matrix(data, shape=C.shape, dtype=np.float32)\n",
    "        Ck_minus = Ck_plus.T  # Consider k words behind\n",
    "        C += Ck_plus + Ck_minus\n",
    "\n",
    "    print(\"Co-occurrence matrix: {:,} words x {:,} words\".format(*C.shape))\n",
    "    print(\"  {:.02g} nonzero elements\".format(C.nnz))\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a toy corpus to see how this works. With a window of 1, we should see co-occurrence counts for each pair of neighboring words:  \n",
    "`(<s>, nlp)`,  \n",
    "`(nlp, class)`,  \n",
    "`(class, is)`,  \n",
    "and so on - as well as their reversed versions (remember, C is symmetric!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Show co-occurrence on a toy corpus\n",
    "toy_corpus = [\n",
    "    \"nlp class is awesome\",\n",
    "    \"nlp class is fun\"\n",
    "]\n",
    "\n",
    "toy_tokens = list(utils.flatten(s.split() for s in toy_corpus))\n",
    "toy_vocab = vocabulary.Vocabulary(toy_tokens)\n",
    "# sentence_to_ids adds \"<s>\" and \"</s>\"\n",
    "toy_token_ids = list(utils.flatten(toy_vocab.sentence_to_ids(s.split()) \n",
    "                     for s in toy_corpus))\n",
    "\n",
    "# Here's the important part\n",
    "toy_C = cooccurrence_matrix(toy_token_ids, toy_vocab.size, K=1)\n",
    "\n",
    "toy_labels = toy_vocab.ordered_words()\n",
    "utils.pretty_print_matrix(toy_C.toarray(), rows=toy_labels, cols=toy_labels, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Word Vectors\n",
    "\n",
    "In order to go from our co-occurrence matrix to word vectors, we need to do two things:\n",
    "\n",
    "- First, convert to **PPMI** to reduce the impact of common words.\n",
    "- Compute the **SVD**, and extract our vectors.\n",
    "\n",
    "### PPMI\n",
    "\n",
    "PPMI stands for Positive [Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information), which you've seen on [Assignment 1](../../assignment/a1/information_theory.ipynb#Pointwise-Mutual-Information). PMI is a generalization of the idea of correlation, but for arbitrary variables. Here, we're interested in the correlation between word $i$ and word $j$, where we take the samples to be all the word-word pairs in our corpus.  \n",
    "Positive just means we'll truncate at zero: $\\text{PPMI}(i,j) = \\max(0, \\text{PMI}(i,j))$\n",
    "\n",
    "We'll apply PPMI as a transformation of our counts matrix. First, compute probabilities:\n",
    "$$ P(i,j) = \\frac{C(i,j)}{\\sum_{k,l} C(k,l)} = \\frac{C_{ij}}{Z}$$\n",
    "$$ P(i) = \\frac{\\sum_{k} C(i,k)}{\\sum_{k,l} C(k,l)} = \\frac{Z_i}{Z}$$\n",
    "\n",
    "Then compute PMI:\n",
    "$$ \\text{PMI}(i,j) = \\log \\frac{P(i,j)}{P(i)P(j)} = \\log \\frac{C_{ij} \\cdot Z}{Z_i \\cdot Z_j} $$\n",
    "\n",
    "Then truncate to ignore negatively-correlated pairs:\n",
    "$$\\text{PPMI}(i,j) = \\max(0, \\text{PMI}(i,j))$$\n",
    "\n",
    "#### Note on Sparse Matricies\n",
    "\n",
    "In order to compute PPMI, we'll need to \"unpack\" the nonzero elements. Recall when we were constructing it, we constructed a list of `(values, (indices))`:\n",
    "```\n",
    "data = (np.ones_like(i), (i,j))  # values, indices\n",
    "```\n",
    "We'll do the inverse of this here, then transform all the values in parallel, then pack them back into a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def PPMI(C):\n",
    "    \"\"\"Tranform a counts matrix to PPMI.\n",
    "    \n",
    "    Args:\n",
    "      C: scipy.sparse.csc_matrix of counts C_ij\n",
    "    \n",
    "    Returns:\n",
    "      (scipy.sparse.csc_matrix) PPMI(C) as defined above\n",
    "    \"\"\"\n",
    "    Z = float(C.sum())  # total counts\n",
    "    # sum each column (along rows)\n",
    "    Zc = np.array(C.sum(axis=0), dtype=np.float64).flatten()\n",
    "    # sum each row (along columns)\n",
    "    Zr = np.array(C.sum(axis=1), dtype=np.float64).flatten()\n",
    "    \n",
    "    # Get indices of relevant elements\n",
    "    ii, jj = C.nonzero()  # row, column indices\n",
    "    Cij = np.array(C[ii,jj], dtype=np.float64).flatten()\n",
    "    \n",
    "    ##\n",
    "    # PMI equation\n",
    "    pmi = np.log(Cij * Z / (Zr[ii] * Zc[jj]))\n",
    "    ##\n",
    "    # Truncate to positive only\n",
    "    ppmi = np.maximum(0, pmi)  # take positive only\n",
    "    \n",
    "    # Re-format as sparse matrix\n",
    "    ret = scipy.sparse.csc_matrix((ppmi, (ii,jj)), shape=C.shape,\n",
    "                                  dtype=np.float64)\n",
    "    ret.eliminate_zeros()  # remove zeros\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this does on our toy corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils.pretty_print_matrix(PPMI(toy_C).toarray(), rows=toy_labels, \n",
    "                          cols=toy_labels, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SVD\n",
    "\n",
    "Recall from async that the [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) decomposes an $m \\times n$ matrix $X$ as:\n",
    "\n",
    "$$ X = UDV^T $$ \n",
    "\n",
    "where $U$ is $m\\times m$, $D$ is $m \\times n$, and $V$ is $n \\times n$, $U$ and $V$ are orthonormal matricies, and $D$ is diagonal. \n",
    "\n",
    "Conventionally, we take the diagonal elements of $D$ to be in order, so $D_{00}$ is the largest singular value, and so on. Then we can take the first $d$ columns of $U$ to be our word vector representations.\n",
    "\n",
    "This is a very standard algorithm with many implementations. We'll use the one in [`sklearn.decomposition.TruncatedSVD`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html), which will only compute the $d \\ll |V|$ components we need.\n",
    "\n",
    "#### Note: known Anaconda bug\n",
    "\n",
    "There's a [known bug](https://github.com/BVLC/caffe/issues/3884) with Anaconda's configuration of some linear algebra libraries. If your Python kernel crashes on running the SVD, open a terminal and run:\n",
    "```\n",
    "conda install mkl\n",
    "```\n",
    "That should re-link the packages. You may need to restart your kernel for it to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "def SVD(X, d=100):\n",
    "    \"\"\"Returns word vectors from SVD.\n",
    "    \n",
    "    Args:\n",
    "      X: m x n matrix\n",
    "      d: word vector dimension\n",
    "      \n",
    "    Returns:\n",
    "      Wv : m x d matrix, each row is a word vector.\n",
    "    \"\"\"\n",
    "    transformer = TruncatedSVD(n_components=d, random_state=1)\n",
    "    Wv = transformer.fit_transform(X)\n",
    "    # Normalize to unit length\n",
    "    Wv = Wv / np.linalg.norm(Wv, axis=1).reshape([-1,1])\n",
    "    return Wv, transformer.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, applied to our toy corpus. Note that \"fun\" and \"awesome\" appear in identical contexts, so they get identical vector representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 3\n",
    "utils.pretty_print_matrix(SVD(PPMI(toy_C).toarray(), d=d)[0], \n",
    "                          rows=toy_labels, cols=range(d), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can compute our word vectors on our whole corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1\n",
    "d = 25\n",
    "t0 = time.time()\n",
    "C = cooccurrence_matrix(token_ids, vocab.size, K=K)\n",
    "print(\"Computed Co-occurrence matrix in {:s}\".format(utils.pretty_timedelta(since=t0))); t0 = time.time()\n",
    "C_ppmi = PPMI(C)\n",
    "print(\"Computed PPMI in {:s}\".format(utils.pretty_timedelta(since=t0))); t0 = time.time()\n",
    "Wv, _ = SVD(C_ppmi, d=d)\n",
    "print(\"Computed SVD in {:s}\".format(utils.pretty_timedelta(since=t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "For a quick visualization, we can plot the first two dimensions directly. Plotly makes this quite easy, and gives us free hovertext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "hover = HoverTool(tooltips=[(\"word\", \"@desc\")])\n",
    "wztool = WheelZoomTool()\n",
    "fig = bp.figure(plot_width=600, plot_height=600, tools=[hover, wztool, 'pan', 'reset'])\n",
    "fig.toolbar.active_scroll = wztool\n",
    "df = bp.ColumnDataSource(dict(x=Wv[:n,0], y=Wv[:n,1], desc=vocab.ids_to_words(range(n))))\n",
    "fig.circle('x', 'y', source=df)\n",
    "fig.add_layout(LabelSet(x='x', y='y', text='desc', source=df,\n",
    "                        x_offset=2, y_offset=2))\n",
    "bp.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this plot is quite limited. Pick a point and look at the words nearby - do they look related, either syntactically or semantically?\n",
    "\n",
    "Plotting two dimensions directly like this is equivalent to just doing the truncated SVD with $d=2$, which throws away quite a lot of information.\n",
    "\n",
    "## t-SNE\n",
    "\n",
    "To get a better sense of our embedding structure, we can use [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) instead. This is a *non*-linear way of embedding high-dimensional data (like our embedding vectors) into a low dimensional space. It works by preserving local distances (like nearby neighbors), at the expense of some global distortion.\n",
    "\n",
    "The result is no longer a projection, but because it preserves locality  t-SNE is a very useful took to look at **clusters**.\n",
    "\n",
    "\n",
    "*Note: there's also a demo at http://projector.tensorflow.org/, pre-loaded with word2vec vectors.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Running t-SNE in-notebook\n",
    "\n",
    "We recommend using the TensorFlow projector, but you can also run t-SNE directly in the notebook, then plot the points with Plotly or another plotting library.\n",
    "\n",
    "Scikit-learn includes a t-SNE implementation in [`sklearn.manifold.TSNE`](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), but the implementation is slow and tends to crash by using too much (>4 GB) memory.\n",
    "\n",
    "Instead, we'll use the excellent [`bhtsne`](https://github.com/dominiek/python-bhtsne) package. Install with:\n",
    "```\n",
    "sudo apt-get install gcc g++\n",
    "pip install bhtsne\n",
    "```\n",
    "\n",
    "The cell below will take around 2-3 minutes to run on a 2 CPU Cloud Compute instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bhtsne\n",
    "\n",
    "n = 5000  # t-SNE is very slow, so restrict vocab size\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"Running Barnes-Hut t-SNE on word vectors; matrix shape = {:s}\".format(str(Wv.shape)))\n",
    "Wv2 = bhtsne.tsne(Wv[:n])\n",
    "print(\"Transformed in {:s}\".format(utils.pretty_timedelta(since=t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "hover = HoverTool(tooltips=[(\"word\", \"@desc\")])\n",
    "wztool = WheelZoomTool()\n",
    "fig = bp.figure(plot_width=600, plot_height=600, tools=[hover, wztool, 'pan', 'reset'])\n",
    "fig.toolbar.active_scroll = wztool\n",
    "df = bp.ColumnDataSource(dict(x=Wv2[:n,0], y=Wv2[:n,1], desc=vocab.ids_to_words(range(n))))\n",
    "fig.circle('x', 'y', source=df)\n",
    "fig.add_layout(LabelSet(x='x', y='y', text='desc', source=df,\n",
    "                        x_offset=2, y_offset=2))\n",
    "bp.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
